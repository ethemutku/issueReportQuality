{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e984b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import jpype\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4701dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filePath):\n",
    "    #load the dataset\n",
    "    dataset = pandas.read_csv(filePath, encoding=\"ISO-8859-9\", delimiter=\";\")\n",
    "\n",
    "    #clean the spaces at the beginning and end of column names.\n",
    "    dataset.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def filterProject(self, dataset, projectCode):\n",
    "    datasetP = dataset[(dataset[CNAME_PROJECT] == projectCode)]\n",
    "    datasetO = dataset[(dataset[CNAME_PROJECT] != projectCode)]\n",
    "\n",
    "    return datasetO, datasetP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7cecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "zemberek = None\n",
    "\n",
    "# ====================================================================================================++\n",
    "# Start JVM\n",
    "# Change the address below according to the java version and the location of the jar file on your computer.\n",
    "jpype.startJVM(\".../jvm.dll\",\n",
    "               \"-Djava.class.path=zemberek-tum-2.0.jar\", \"-ea\")\n",
    "\n",
    "# set the language as Turkish (as spoken in Turkey)\n",
    "Tr = jpype.JClass(\"net.zemberek.tr.yapi.TurkiyeTurkcesi\")\n",
    "# tr object\n",
    "tr = Tr()\n",
    "# load the Zemberek class\n",
    "Zemberek = jpype.JClass(\"net.zemberek.erisim.Zemberek\")\n",
    "ZemberekC = jpype.JClass(\"net.zemberek.araclar.turkce.YaziIsleyici\")\n",
    "# zemberek object\n",
    "zemberek = Zemberek(tr)\n",
    "# ====================================================================================================++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d142de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = [] # list of stop words to be extracted from the text\n",
    "\n",
    "# These letters are specific to Turkish language. Below is a map for lowering them specifically\n",
    "lower_map_turkish = {\n",
    "    ord(u'I'): u'ı',\n",
    "    ord(u'İ'): u'i',\n",
    "    ord(u'Ç'): u'ç',\n",
    "    ord(u'Ş'): u'ş',\n",
    "    ord(u'Ö'): u'ö',\n",
    "    ord(u'Ü'): u'ü',\n",
    "    ord(u'Ğ'): u'ğ'\n",
    "    }\n",
    "\n",
    "# List of words or word roots indicating related patterns are added in the below list\n",
    "\n",
    "# list of words that do not require morphological analysis\n",
    "ob_non_root = [\"inaktif\", \"olarak\", \"şeklinde\", \"birbirine\", \"defa\", \"ancak\", \"hala\", \"hiç\", \"eski\",\n",
    "               \"ancak\", \"lakin\", \"fakat\", \"rağmen\", \"sadece\", \"inaktif\", \"mükerrer\", \"tekrar\", \"ne\"]\n",
    "eb_non_root = []\n",
    "s2r_non_root = []\n",
    "# list of word roots that require morphological analysis by removing the suffixes\n",
    "ob_root = [\"ek\", \"hata\", \"uyarı\", \"mesaj\", \"sorun\", \"fark\", \"geri\", \"takı\",\n",
    "           \"aşırı\", \"şifre\", 'geç', \"örnek\", \"iptal\", \"şikayet\", \"hal\",\n",
    "           \"uyuş\", \"askı\", \"eksik\", \"deney\", \"don\", \"yut\"]\n",
    "eb_root = [\"bekle\", \"iste\", \"gerek\"]\n",
    "s2r_root = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64ef6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams_based_extractor(text):\n",
    "    \"\"\"\n",
    "    lower the characters in the text, remove all non-word characters and stop words, \n",
    "    then returns the remaining words as features.\n",
    "    \"\"\"\n",
    "    # Remove digits\n",
    "    wordsNoDigit = []\n",
    "    for word in text.split():\n",
    "        wordsNoDigit.append(re.sub('[\\d]+|,|;|\\.', ' ', word))\n",
    "    textNoDigit = ' '.join(wordsNoDigit)\n",
    "\n",
    "    # Remove all non-word characters from the text via the regex[\\W]+,\n",
    "    # Convert the text into lowercase characters\n",
    "    # print(text)\n",
    "    text_tr = textNoDigit.translate(lower_map_turkish)\n",
    "    lowerText = re.sub('[\\W]+', ' ', text_tr.lower())\n",
    "\n",
    "    #remove stopwords\n",
    "    noStopWordsText = [word for word in lowerText.split() if ((word not in stop_word_list) and (len(word) > 1))]\n",
    "    \n",
    "    return ' '.join(noStopWordsText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ba796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_based_extractor(text, include_input):\n",
    "    \"\"\"\n",
    "    morphological analysis based extractor: use zemberek to extract the word root and the suffixes, \n",
    "    and return them to be used as features\n",
    "    \n",
    "    include_input: 0>Return only ma results or patterns results, 1>Return the input text as well\n",
    "    \"\"\"\n",
    "    resultWordList = []\n",
    "    for word in text.split():\n",
    "        if zemberek.kelimeDenetle(word): # Zemberek checks if the word is valid\n",
    "            yanit = zemberek.kelimeCozumle(word) # Zemberek analyzes the word\n",
    "            if yanit:\n",
    "                strSuffixes = str(yanit[0].ekler()).replace('[', '').replace(']', '').replace(' ', '') \n",
    "                rootIsVerb = False\n",
    "                for morpheme in strSuffixes.split(\",\"):\n",
    "                    if morpheme[:4] == \"FIIL\": \n",
    "                        rootIsVerb = True\n",
    "                if rootIsVerb:\n",
    "                    for morpheme in strSuffixes.split(\",\"):\n",
    "                        resultWordList.append(morpheme) # add the morphemes to the list of words to be returned\n",
    "            else:\n",
    "                print(\"{} COUND NOT BE ANALYZED\".format(word))\n",
    "        else:\n",
    "            print(\"{} UNKNOWN WORD\".format(word))\n",
    "    if include_input == 0:\n",
    "        return ' '.join(resultWordList)\n",
    "    else: \n",
    "        return text + \" \" + ' '.join(resultWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18119db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_based_extractor(text, include_input, classification_option, addAllVerbSuffixes):\n",
    "    \"\"\"\n",
    "    after morphological analysis, patterns extractor uses the matching parts of \n",
    "    the morphological analysis results as features\n",
    "    \n",
    "    include_input: 0>Return only ma results or patterns results, 1>Return the input text as well\n",
    "    \n",
    "    classification_option: ob, eb, s2r\n",
    "    \n",
    "    addAllVerbSuffixes: True -> patterns+ma / False: patterns \n",
    "    \"\"\"\n",
    "    resultWordList = []\n",
    "    for word in text.split():\n",
    "        if (classification_option == 'ob') and (word in ob_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif (classification_option == 'eb') and word in (eb_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif (classification_option == 's2r') and word in (s2r_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif zemberek.kelimeDenetle(word): # Zemberek checks if the word is valid\n",
    "            yanit = zemberek.kelimeCozumle(word) # Zemberek analyzes the word\n",
    "            if yanit:\n",
    "                strSuffixes = str(yanit[0].ekler()).replace('[', '').replace(']', '').replace(' ', '')\n",
    "                rootIsVerb = False\n",
    "                for morpheme in strSuffixes.split(\",\"):\n",
    "                    if morpheme[:4] == \"FIIL\":\n",
    "                        rootIsVerb = True\n",
    "                if rootIsVerb:\n",
    "                    for morpheme in strSuffixes.split(\",\"):\n",
    "                        if addAllVerbSuffixes:\n",
    "                            resultWordList.append(morpheme) # add the morphemes to the list of words to be returned\n",
    "                        if (classification_option == 'ob') and (morpheme == \"FIIL_OLUMSUZLUK_ME\"): # OB pattern -me -ma\n",
    "                            resultWordList.append(\"FIIL_OLUMSUZLUK_ME\")\n",
    "                        if (classification_option == 'eb') and (morpheme == \"ISIM_BULUNMA_LI\"): # EB pattern -meli\n",
    "                            resultWordList.append(\"FIIL_DONUSUM_ME\")\n",
    "                            resultWordList.append(\"ISIM_BULUNMA_LI\")\n",
    "                        if (classification_option == 'eb') and (morpheme == \"FIIL_YETENEK_EBIL\"):  # EB pattern -ebil\n",
    "                            resultWordList.append(\"FIIL_YETENEK_EBIL\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"ISIM_KALMA_DE\"):  # S2R pattern -de -da\n",
    "                            resultWordList.append(\"ISIM_KALMA_DE\")\n",
    "                            if prevMorpheme == \"FIIL_MASTAR_MEK\":\n",
    "                                resultWordList.append(\"FIIL_MASTAR_MEK\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"IMEK_ZAMAN_KEN\"):  # S2R pattern -ken\n",
    "                            resultWordList.append(\"IMEK_ZAMAN_KEN\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"FIIL_GECMISZAMAN_MIS\"):  # S2R pattern -miş\n",
    "                            resultWordList.append(\"FIIL_GECMISZAMAN_MIS\")\n",
    "                        prevMorpheme = morpheme\n",
    "                if (classification_option == 'ob') and (str(yanit[0].kok()).split(\" \")[0] in ob_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])\n",
    "                if (classification_option == 'eb') and (str(yanit[0].kok()).split(\" \")[0] in eb_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])\n",
    "                if (classification_option == 's2r') and (str(yanit[0].kok()).split(\" \")[0] in s2r_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])                    \n",
    "            else:\n",
    "                print(\"{} COULD NOT BE ANALYZED\".format(word))\n",
    "        else:\n",
    "            print(\"{} UNKNOWN WORD\".format(word))\n",
    "    if include_input == 0:\n",
    "        print(\"patterns_based_extractor0:\", ' '.join(resultWordList))\n",
    "        return ' '.join(resultWordList)\n",
    "    else: \n",
    "        return text + \" \" + ' '.join(resultWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bab0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfileName = \"data/issueReports.csv\"\n",
    "\n",
    "dataset = load(inputfileName)\n",
    "print(\"Dataset length: \" + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb59a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header names in the input file\n",
    "CNAME_DESCRIPTION = \"DESCRIPTION\"\n",
    "CNAME_QUAL_FLAG = \"OB\"\n",
    "# function inputs\n",
    "classification_option = 'ob' #ob, eb, s2r\n",
    "include_input = 0 # 0: Return only ma/patterns results, 1: Return the input text as well\n",
    "\n",
    "features = 'n_grams' # options> 'n_grams', 'ma', 'patterns', 'n_grams+ma', 'n_grams+patterns', 'n_grams+ma+patterns'\n",
    "addAllVerbSuffixes = False # True: patterns+ma\n",
    "dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(n_grams_based_extractor)\n",
    "if features == 'n_grams':\n",
    "    pass\n",
    "elif features == 'ma':\n",
    "    include_input = 0 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(ma_based_extractor, args=(include_input,))\n",
    "elif features == 'patterns':\n",
    "    include_input = 0 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes,))\n",
    "elif features == 'patterns+ma':\n",
    "    include_input = 0 \n",
    "    addAllVerbSuffixes = True\n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes,))\n",
    "elif features == 'n_grams+ma':\n",
    "    include_input = 1 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(ma_based_extractor, args=(include_input,))\n",
    "elif features == 'n_grams+patterns':\n",
    "    include_input = 1 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes,))\n",
    "elif features == 'n_grams+ma+patterns':\n",
    "    include_input = 1 \n",
    "    #dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(ma_based_extractor, args=(include_input,))\n",
    "    addAllVerbSuffixes = True\n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187fb49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes =  list(set(dataset[CNAME_QUAL_FLAG]))\n",
    "\n",
    "# Handling class imbalance\n",
    "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\", \n",
    "                                                  classes = np.unique(dataset[CNAME_QUAL_FLAG]), \n",
    "                                                  y = dataset[CNAME_QUAL_FLAG])\n",
    "print(class_weights)\n",
    "\n",
    "weights={}\n",
    "for index, weight in enumerate(class_weights):\n",
    "    weights[index]=weight\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9169ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectCodeList = [\"UK0811\", \"UK1440\", \"UK0137\", \"UK0451\", \"UK0698\", \"UK0213\", \"UK0196\", \"UK0440\", \"UK0199\", \"UK1148\"] \n",
    "results = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "#for train, test in kfold.split(dataset[CNAME_DESCRIPTION], dataset[CNAME_QUAL_FLAG]):\n",
    "for projectCode in projectCodeList: \n",
    "    datasetTrain, datasetTest = filterProject(dataset, projectCode) \n",
    "    print(\"Dataset length: \" + str(len(datasetTrain)), len(datasetTest)) \n",
    "    X_train = datasetTrain[CNAME_DESCRIPTION].values \n",
    "    y_train = datasetTrain[CNAME_QUAL_FLAG].values \n",
    "\n",
    "    X_test = datasetTest[CNAME_DESCRIPTION].values \n",
    "    y_test = datasetTest[CNAME_QUAL_FLAG].values\n",
    "\n",
    "    print(len(y_train), len(y_test)) \n",
    "    \n",
    "    # summarize train and test composition\n",
    "    train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "    test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "    print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "    \n",
    "    # Pre Processing \n",
    "    trn, val, preproc = ktrain.text.texts_from_array(x_train=X_train, y_train=y_train,\n",
    "                                                     x_test=X_test, y_test=y_test,\n",
    "                                                     class_names=classes,\n",
    "                                                     val_pct=0.1,\n",
    "                                                     max_features=1000,\n",
    "                                                     maxlen=100,\n",
    "                                                     preprocess_mode='distilbert',\n",
    "                                                     ngram_range=(1,2))\n",
    "\n",
    "    # Model \n",
    "    model = ktrain.text.text_classifier('distilbert', train_data=trn, preproc=preproc)\n",
    "    \n",
    "    learner =  ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=16)\n",
    "\n",
    "    # Applying the weights\n",
    "    learner.autofit(1e-4, 2, class_weight = weights)\n",
    "\n",
    "    predictor = ktrain.get_predictor(learner.model, preproc)\n",
    "    yhat = []\n",
    "    for x_t in X_test:\n",
    "        yhat.append(predictor.predict(x_t))\n",
    "\n",
    "    print(accuracy_score(y_test, yhat))\n",
    "    print(precision_score(y_test, yhat))\n",
    "    print(recall_score(y_test, yhat))\n",
    "    print(f1_score(y_test, yhat))\n",
    "    results[0] = results[0] + accuracy_score(y_test, yhat)\n",
    "    results[1] = results[1] + precision_score(y_test, yhat)\n",
    "    results[2] = results[2] + recall_score(y_test, yhat)\n",
    "    results[3] = results[3] + f1_score(y_test, yhat)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
