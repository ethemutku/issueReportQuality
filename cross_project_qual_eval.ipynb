{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e984b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import jpype\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4701dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filePath):\n",
    "    #load the dataset\n",
    "    dataset = pandas.read_csv(filePath, encoding=\"ISO-8859-9\", delimiter=\";\")\n",
    "\n",
    "    #clean the spaces at the beginning and end of column names.\n",
    "    dataset.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def filterProject(self, dataset, projectCode):\n",
    "    datasetP = dataset[(dataset[CNAME_PROJECT] == projectCode)]\n",
    "    datasetO = dataset[(dataset[CNAME_PROJECT] != projectCode)]\n",
    "\n",
    "    return datasetO, datasetP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7cecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "zemberek = None\n",
    "\n",
    "# ====================================================================================================++\n",
    "# Start JVM\n",
    "# Change the address below according to the java version and the location of the jar file on your computer.\n",
    "jpype.startJVM(\".../jvm.dll\",\n",
    "               \"-Djava.class.path=zemberek-tum-2.0.jar\", \"-ea\")\n",
    "\n",
    "# set the language as Turkish (as spoken in Turkey)\n",
    "Tr = jpype.JClass(\"net.zemberek.tr.yapi.TurkiyeTurkcesi\")\n",
    "# tr object\n",
    "tr = Tr()\n",
    "# load the Zemberek class\n",
    "Zemberek = jpype.JClass(\"net.zemberek.erisim.Zemberek\")\n",
    "ZemberekC = jpype.JClass(\"net.zemberek.araclar.turkce.YaziIsleyici\")\n",
    "# zemberek object\n",
    "zemberek = Zemberek(tr)\n",
    "# ====================================================================================================++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d142de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = [] # list of stop words to be extracted from the text\n",
    "\n",
    "# These letters are specific to Turkish language. Below is a map for lowering them specifically\n",
    "lower_map_turkish = {\n",
    "    ord(u'I'): u'ı',\n",
    "    ord(u'İ'): u'i',\n",
    "    ord(u'Ç'): u'ç',\n",
    "    ord(u'Ş'): u'ş',\n",
    "    ord(u'Ö'): u'ö',\n",
    "    ord(u'Ü'): u'ü',\n",
    "    ord(u'Ğ'): u'ğ'\n",
    "    }\n",
    "\n",
    "# List of words or word roots indicating related patterns are added in the below list\n",
    "\n",
    "# list of words that do not require morphological analysis\n",
    "ob_non_root = [\"inaktif\", \"olarak\", \"şeklinde\", \"birbirine\", \"defa\", \"ancak\", \"hala\", \"hiç\", \"eski\",\n",
    "               \"ancak\", \"lakin\", \"fakat\", \"rağmen\", \"sadece\", \"inaktif\", \"mükerrer\", \"tekrar\", \"ne\"]\n",
    "eb_non_root = []\n",
    "s2r_non_root = []\n",
    "# list of word roots that require morphological analysis by removing the suffixes\n",
    "ob_root = [\"ek\", \"hata\", \"uyarı\", \"mesaj\", \"sorun\", \"fark\", \"geri\", \"takı\",\n",
    "           \"aşırı\", \"şifre\", 'geç', \"örnek\", \"iptal\", \"şikayet\", \"hal\",\n",
    "           \"uyuş\", \"askı\", \"eksik\", \"deney\", \"don\", \"yut\"]\n",
    "eb_root = [\"bekle\", \"iste\", \"gerek\"]\n",
    "s2r_root = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64ef6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams_based_extractor(text):\n",
    "    \"\"\"\n",
    "    lower the characters in the text, remove all non-word characters and stop words, \n",
    "    then returns the remaining words as features.\n",
    "    \"\"\"\n",
    "    # Remove digits\n",
    "    wordsNoDigit = []\n",
    "    for word in text.split():\n",
    "        wordsNoDigit.append(re.sub('[\\d]+|,|;|\\.', ' ', word))\n",
    "    textNoDigit = ' '.join(wordsNoDigit)\n",
    "\n",
    "    # Remove all non-word characters from the text via the regex[\\W]+,\n",
    "    # Convert the text into lowercase characters\n",
    "    # print(text)\n",
    "    text_tr = textNoDigit.translate(lower_map_turkish)\n",
    "    lowerText = re.sub('[\\W]+', ' ', text_tr.lower())\n",
    "\n",
    "    #remove stopwords\n",
    "    noStopWordsText = [word for word in lowerText.split() if ((word not in stop_word_list) and (len(word) > 1))]\n",
    "    \n",
    "    return ' '.join(noStopWordsText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ba796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_based_extractor(text, include_input):\n",
    "    \"\"\"\n",
    "    morphological analysis based extractor: use zemberek to extract the word root and the suffixes, \n",
    "    and return them to be used as features\n",
    "    \n",
    "    include_input: 0>Return only ma results or patterns results, 1>Return the input text as well\n",
    "    \"\"\"\n",
    "    resultWordList = []\n",
    "    for word in text.split():\n",
    "        if zemberek.kelimeDenetle(word): # Zemberek checks if the word is valid\n",
    "            yanit = zemberek.kelimeCozumle(word) # Zemberek analyzes the word\n",
    "            if yanit:\n",
    "                strSuffixes = str(yanit[0].ekler()).replace('[', '').replace(']', '').replace(' ', '') \n",
    "                rootIsVerb = False\n",
    "                for morpheme in strSuffixes.split(\",\"):\n",
    "                    if morpheme[:4] == \"FIIL\": \n",
    "                        rootIsVerb = True\n",
    "                if rootIsVerb:\n",
    "                    for morpheme in strSuffixes.split(\",\"):\n",
    "                        resultWordList.append(morpheme) # add the morphemes to the list of words to be returned\n",
    "            else:\n",
    "                print(\"{} COUND NOT BE ANALYZED\".format(word))\n",
    "        else:\n",
    "            print(\"{} UNKNOWN WORD\".format(word))\n",
    "    if include_input == 0:\n",
    "        return ' '.join(resultWordList)\n",
    "    else: \n",
    "        return text + \" \" + ' '.join(resultWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18119db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_based_extractor(text, include_input, classification_option, addAllVerbSuffixes):\n",
    "    \"\"\"\n",
    "    after morphological analysis, patterns extractor uses the matching parts of \n",
    "    the morphological analysis results as features\n",
    "    \n",
    "    include_input: 0>Return only ma results or patterns results, 1>Return the input text as well\n",
    "    \n",
    "    classification_option: ob, eb, s2r\n",
    "    \n",
    "    addAllVerbSuffixes: True -> patterns+ma / False: patterns \n",
    "    \"\"\"\n",
    "    resultWordList = []\n",
    "    for word in text.split():\n",
    "        if (classification_option == 'ob') and (word in ob_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif (classification_option == 'eb') and word in (eb_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif (classification_option == 's2r') and word in (s2r_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif zemberek.kelimeDenetle(word): # Zemberek checks if the word is valid\n",
    "            yanit = zemberek.kelimeCozumle(word) # Zemberek analyzes the word\n",
    "            if yanit:\n",
    "                strSuffixes = str(yanit[0].ekler()).replace('[', '').replace(']', '').replace(' ', '')\n",
    "                rootIsVerb = False\n",
    "                for morpheme in strSuffixes.split(\",\"):\n",
    "                    if morpheme[:4] == \"FIIL\":\n",
    "                        rootIsVerb = True\n",
    "                if rootIsVerb:\n",
    "                    for morpheme in strSuffixes.split(\",\"):\n",
    "                        if addAllVerbSuffixes:\n",
    "                            resultWordList.append(morpheme) # add the morphemes to the list of words to be returned\n",
    "                        if (classification_option == 'ob') and (morpheme == \"FIIL_OLUMSUZLUK_ME\"): # OB pattern -me -ma\n",
    "                            resultWordList.append(\"FIIL_OLUMSUZLUK_ME\")\n",
    "                        if (classification_option == 'eb') and (morpheme == \"ISIM_BULUNMA_LI\"): # EB pattern -meli\n",
    "                            resultWordList.append(\"FIIL_DONUSUM_ME\")\n",
    "                            resultWordList.append(\"ISIM_BULUNMA_LI\")\n",
    "                        if (classification_option == 'eb') and (morpheme == \"FIIL_YETENEK_EBIL\"):  # EB pattern -ebil\n",
    "                            resultWordList.append(\"FIIL_YETENEK_EBIL\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"ISIM_KALMA_DE\"):  # S2R pattern -de -da\n",
    "                            resultWordList.append(\"ISIM_KALMA_DE\")\n",
    "                            if prevMorpheme == \"FIIL_MASTAR_MEK\":\n",
    "                                resultWordList.append(\"FIIL_MASTAR_MEK\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"IMEK_ZAMAN_KEN\"):  # S2R pattern -ken\n",
    "                            resultWordList.append(\"IMEK_ZAMAN_KEN\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"FIIL_GECMISZAMAN_MIS\"):  # S2R pattern -miş\n",
    "                            resultWordList.append(\"FIIL_GECMISZAMAN_MIS\")\n",
    "                        prevMorpheme = morpheme\n",
    "                if (classification_option == 'ob') and (str(yanit[0].kok()).split(\" \")[0] in ob_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])\n",
    "                if (classification_option == 'eb') and (str(yanit[0].kok()).split(\" \")[0] in eb_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])\n",
    "                if (classification_option == 's2r') and (str(yanit[0].kok()).split(\" \")[0] in s2r_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])                    \n",
    "            else:\n",
    "                print(\"{} COULD NOT BE ANALYZED\".format(word))\n",
    "        else:\n",
    "            print(\"{} UNKNOWN WORD\".format(word))\n",
    "    if include_input == 0:\n",
    "        print(\"patterns_based_extractor0:\", ' '.join(resultWordList))\n",
    "        return ' '.join(resultWordList)\n",
    "    else: \n",
    "        return text + \" \" + ' '.join(resultWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bab0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfileName = \"data/issueReports.csv\"\n",
    "\n",
    "dataset = load(inputfileName)\n",
    "print(\"Dataset length: \" + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb59a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header names in the input file\n",
    "CNAME_DESCRIPTION = \"DESCRIPTION\"\n",
    "CNAME_QUAL_FLAG = \"OB\"\n",
    "# function inputs\n",
    "classification_option = 'ob' #ob, eb, s2r\n",
    "include_input = 0 # 0: Return only ma/patterns results, 1: Return the input text as well\n",
    "\n",
    "features = 'n_grams' # options> 'n_grams', 'ma', 'patterns', 'n_grams+ma', 'n_grams+patterns', 'n_grams+ma+patterns'\n",
    "addAllVerbSuffixes = False # True: patterns+ma\n",
    "dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(n_grams_based_extractor)\n",
    "if features == 'n_grams':\n",
    "    pass\n",
    "elif features == 'ma':\n",
    "    include_input = 0 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(ma_based_extractor, args=(include_input,))\n",
    "elif features == 'patterns':\n",
    "    include_input = 0 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes,))\n",
    "elif features == 'patterns+ma':\n",
    "    include_input = 0 \n",
    "    addAllVerbSuffixes = True\n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes,))\n",
    "elif features == 'n_grams+ma':\n",
    "    include_input = 1 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(ma_based_extractor, args=(include_input,))\n",
    "elif features == 'n_grams+patterns':\n",
    "    include_input = 1 \n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes,))\n",
    "elif features == 'n_grams+ma+patterns':\n",
    "    include_input = 1 \n",
    "    #dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(ma_based_extractor, args=(include_input,))\n",
    "    addAllVerbSuffixes = True\n",
    "    dataset[CNAME_DESCRIPTION] = dataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, \n",
    "                                                                  args=(include_input, classification_option, addAllVerbSuffixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187fb49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectCode = 'PRJCODE' # The selected project code is used for validation, the remaining as the training set\n",
    "datasetTrain, datasetTest = filterProject(dataset, projectCode)\n",
    "print(\"Dataset length: \" + str(len(datasetTrain)), len(datasetTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9169ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasetTrain[CNAME_DESCRIPTION].values\n",
    "Y = datasetTrain[CNAME_QUAL_FLAG].values\n",
    "seed = 7\n",
    "X_validation = datasetTest[CNAME_DESCRIPTION].values\n",
    "Y_validation = datasetTest[CNAME_QUAL_FLAG].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22646b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "voc = vectorizer.vocabulary_\n",
    "vectorizer_test = TfidfVectorizer(ngram_range=(1,2), vocabulary=voc)\n",
    "X_tfidf_validation = vectorizer_test.fit_transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e71a3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X_train, y_train = X_tfidf, Y\n",
    "# configure the cross-validation procedure\n",
    "cv_inner = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# define the model\n",
    "model = LinearSVC(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74155919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "space = dict()\n",
    "space['C'] = [0.01, 0.1, 1, 10, 100]\n",
    "# define search\n",
    "search = model_selection.GridSearchCV(model, space, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "# execute search\n",
    "result = search.fit(X_train, y_train)\n",
    "# get the best performing model fit on the whole training set\n",
    "best_model = result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19942c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on the hold out dataset\n",
    "yhat = best_model.predict(X_tfidf_validation)\n",
    "# evaluate the model\n",
    "acc = accuracy_score(Y_validation, yhat)\n",
    "print(accuracy_score(Y_validation, yhat))\n",
    "print(confusion_matrix(Y_validation, yhat))\n",
    "print(classification_report(Y_validation, yhat, digits=4))\n",
    "# report progress\n",
    "print('>acc=%.3f, best=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
