{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e984b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import jpype\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4701dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filePath):\n",
    "    #load the dataset\n",
    "    dataset = pandas.read_csv(filePath, encoding=\"ISO-8859-9\", delimiter=\";\")\n",
    "\n",
    "    #clean the spaces at the beginning and end of column names.\n",
    "    dataset.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7cecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "zemberek = None\n",
    "\n",
    "# ====================================================================================================++\n",
    "# Start JVM\n",
    "# Change the address below according to the java version and the location of the jar file on your computer.\n",
    "jpype.startJVM(\".../jvm.dll\",\n",
    "               \"-Djava.class.path=zemberek-tum-2.0.jar\", \"-ea\")\n",
    "\n",
    "# set the language as Turkish (as spoken in Turkey)\n",
    "Tr = jpype.JClass(\"net.zemberek.tr.yapi.TurkiyeTurkcesi\")\n",
    "# tr object\n",
    "tr = Tr()\n",
    "# load the Zemberek class\n",
    "Zemberek = jpype.JClass(\"net.zemberek.erisim.Zemberek\")\n",
    "ZemberekC = jpype.JClass(\"net.zemberek.araclar.turkce.YaziIsleyici\")\n",
    "# zemberek object\n",
    "zemberek = Zemberek(tr)\n",
    "# ====================================================================================================++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d142de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = [] # list of stop words to be extracted from the text\n",
    "\n",
    "# These letters are specific to Turkish language. Below is a map for lowering them specifically\n",
    "lower_map_turkish = {\n",
    "    ord(u'I'): u'ı',\n",
    "    ord(u'İ'): u'i',\n",
    "    ord(u'Ç'): u'ç',\n",
    "    ord(u'Ş'): u'ş',\n",
    "    ord(u'Ö'): u'ö',\n",
    "    ord(u'Ü'): u'ü',\n",
    "    ord(u'Ğ'): u'ğ'\n",
    "    }\n",
    "\n",
    "# List of words or word roots indicating related patterns are added in the below list\n",
    "\n",
    "# list of words that do not require morphological analysis\n",
    "ob_non_root = [\"inaktif\", \"olarak\", \"şeklinde\", \"birbirine\", \"defa\", \"ancak\", \"hala\", \"hiç\", \"eski\",\n",
    "               \"ancak\", \"lakin\", \"fakat\", \"rağmen\", \"sadece\", \"inaktif\", \"mükerrer\", \"tekrar\", \"ne\"]\n",
    "eb_non_root = []\n",
    "s2r_non_root = []\n",
    "# list of word roots that require morphological analysis by removing the suffixes\n",
    "ob_root = [\"ek\", \"hata\", \"uyarı\", \"mesaj\", \"sorun\", \"fark\", \"geri\", \"takı\",\n",
    "           \"aşırı\", \"şifre\", 'geç', \"örnek\", \"iptal\", \"şikayet\", \"hal\",\n",
    "           \"uyuş\", \"askı\", \"eksik\", \"deney\", \"don\", \"yut\"]\n",
    "eb_root = [\"bekle\", \"iste\", \"gerek\"]\n",
    "s2r_root = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64ef6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams_based_extractor(text):\n",
    "    \"\"\"\n",
    "    lower the characters in the text, remove all non-word characters and stop words, \n",
    "    then returns the remaining words as features.\n",
    "    \"\"\"\n",
    "    # Remove digits\n",
    "    wordsNoDigit = []\n",
    "    for word in text.split():\n",
    "        wordsNoDigit.append(re.sub('[\\d]+|,|;|\\.', ' ', word))\n",
    "    textNoDigit = ' '.join(wordsNoDigit)\n",
    "\n",
    "    # Remove all non-word characters from the text via the regex[\\W]+,\n",
    "    # Convert the text into lowercase characters\n",
    "    # print(text)\n",
    "    text_tr = textNoDigit.translate(lower_map_turkish)\n",
    "    lowerText = re.sub('[\\W]+', ' ', text_tr.lower())\n",
    "\n",
    "    #remove stopwords\n",
    "    noStopWordsText = [word for word in lowerText.split() if ((word not in stop_word_list) and (len(word) > 1))]\n",
    "    \n",
    "    return ' '.join(noStopWordsText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ba796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_based_extractor(text, include_input):\n",
    "    \"\"\"\n",
    "    morphological analysis based extractor: use zemberek to extract the word root and the suffixes, \n",
    "    and return them to be used as features\n",
    "    \n",
    "    include_input: 0>Return only ma results or patterns results, 1>Return the input text as well\n",
    "    \"\"\"\n",
    "    resultWordList = []\n",
    "    for word in text.split():\n",
    "        if zemberek.kelimeDenetle(word): # Zemberek checks if the word is valid\n",
    "            yanit = zemberek.kelimeCozumle(word) # Zemberek analyzes the word\n",
    "            if yanit:\n",
    "                strSuffixes = str(yanit[0].ekler()).replace('[', '').replace(']', '').replace(' ', '') \n",
    "                rootIsVerb = False\n",
    "                for morpheme in strSuffixes.split(\",\"):\n",
    "                    if morpheme[:4] == \"FIIL\": \n",
    "                        rootIsVerb = True\n",
    "                if rootIsVerb:\n",
    "                    for morpheme in strSuffixes.split(\",\"):\n",
    "                        resultWordList.append(morpheme) # add the morphemes to the list of words to be returned\n",
    "            else:\n",
    "                print(\"{} COUND NOT BE ANALYZED\".format(word))\n",
    "        else:\n",
    "            print(\"{} UNKNOWN WORD\".format(word))\n",
    "    if include_input == 0:\n",
    "        return ' '.join(resultWordList)\n",
    "    else: \n",
    "        return text + \" \" + ' '.join(resultWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18119db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_based_extractor(text, include_input, classification_option):\n",
    "    \"\"\"\n",
    "    after morphological analysis, patterns extractor uses the matching parts of \n",
    "    the morphological analysis results as features\n",
    "    \n",
    "    include_input: 0>Return only ma results or patterns results, 1>Return the input text as well\n",
    "    \n",
    "    classification_option: ob, eb, s2r\n",
    "    \"\"\"\n",
    "    resultWordList = []\n",
    "    for word in text.split():\n",
    "        if (classification_option == 'ob') and (word in ob_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif (classification_option == 'eb') and word in (eb_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif (classification_option == 's2r') and word in (s2r_non_root):\n",
    "            resultWordList.append(word)\n",
    "        elif zemberek.kelimeDenetle(word): # Zemberek checks if the word is valid\n",
    "            yanit = zemberek.kelimeCozumle(word) # Zemberek analyzes the word\n",
    "            if yanit:\n",
    "                strSuffixes = str(yanit[0].ekler()).replace('[', '').replace(']', '').replace(' ', '')\n",
    "                rootIsVerb = False\n",
    "                for morpheme in strSuffixes.split(\",\"):\n",
    "                    if morpheme[:4] == \"FIIL\":\n",
    "                        rootIsVerb = True\n",
    "                if rootIsVerb:\n",
    "                    for morpheme in strSuffixes.split(\",\"):\n",
    "                        if (classification_option == 'ob') and (morpheme == \"FIIL_OLUMSUZLUK_ME\"): # OB pattern -me -ma\n",
    "                            resultWordList.append(\"FIIL_OLUMSUZLUK_ME\")\n",
    "                        if (classification_option == 'eb') and (morpheme == \"ISIM_BULUNMA_LI\"): # EB pattern -meli\n",
    "                            resultWordList.append(\"FIIL_DONUSUM_ME\")\n",
    "                            resultWordList.append(\"ISIM_BULUNMA_LI\")\n",
    "                        if (classification_option == 'eb') and (morpheme == \"FIIL_YETENEK_EBIL\"):  # EB pattern -ebil\n",
    "                            resultWordList.append(\"FIIL_YETENEK_EBIL\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"ISIM_KALMA_DE\"):  # S2R pattern -de -da\n",
    "                            resultWordList.append(\"ISIM_KALMA_DE\")\n",
    "                            if prevMorpheme == \"FIIL_MASTAR_MEK\":\n",
    "                                resultWordList.append(\"FIIL_MASTAR_MEK\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"IMEK_ZAMAN_KEN\"):  # S2R pattern -ken\n",
    "                            resultWordList.append(\"IMEK_ZAMAN_KEN\")\n",
    "                        if (classification_option == 's2r') and (morpheme == \"FIIL_GECMISZAMAN_MIS\"):  # S2R pattern -miş\n",
    "                            resultWordList.append(\"FIIL_GECMISZAMAN_MIS\")\n",
    "                        prevMorpheme = morpheme\n",
    "                if (classification_option == 'ob') and (str(yanit[0].kok()).split(\" \")[0] in ob_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])\n",
    "                if (classification_option == 'eb') and (str(yanit[0].kok()).split(\" \")[0] in eb_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])\n",
    "                if (classification_option == 's2r') and (str(yanit[0].kok()).split(\" \")[0] in s2r_root):\n",
    "                    resultWordList.append(str(yanit[0].kok()).split(\" \")[0])                    \n",
    "            else:\n",
    "                print(\"{} COULD NOT BE ANALYZED\".format(word))\n",
    "        else:\n",
    "            print(\"{} UNKNOWN WORD\".format(word))\n",
    "    if include_input == 0:\n",
    "        return ' '.join(resultWordList)\n",
    "    else: \n",
    "        return text + \" \" + ' '.join(resultWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bab0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfileName = \"data/olay_2020EkimKasim_part1.csv\"\n",
    "testfileName = \"data/olay_2022-full.csv\"\n",
    "modelsDirectoryName = \"models\"\n",
    "\n",
    "trainDataset = load(trainfileName)\n",
    "testDataset = load(testfileName)\n",
    "print(\"Train dataset length: \" + str(len(trainDataset)))\n",
    "print(\"Test dataset length: \" + str(len(testDataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb59a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNAME_DESCRIPTION = \"DESCRIPTION\"\n",
    "CNAME_VALID_FLAG = \"S2R\"\n",
    "classification_option = 's2r' #ob, eb, s2r\n",
    "include_input = 1 # 0: Return only ma results or patterns results, 1: Return the input text as well\n",
    "trainDataset[CNAME_DESCRIPTION] = trainDataset[CNAME_DESCRIPTION].apply(n_grams_based_extractor)\n",
    "trainDataset[CNAME_DESCRIPTION] = trainDataset[CNAME_DESCRIPTION].apply(ma_based_extractor, args=(include_input,))\n",
    "trainDataset[CNAME_DESCRIPTION] = trainDataset[CNAME_DESCRIPTION].apply(patterns_based_extractor, args=(include_input, classification_option,))\n",
    "\n",
    "X_train = trainDataset[CNAME_DESCRIPTION].values\n",
    "Y_train = trainDataset[CNAME_VALID_FLAG].values\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fb49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset[CNAME_DESCRIPTION] = testDataset[CNAME_DESCRIPTION].apply(filterWithNgramsPatternsAndMA, args=(classification_option,))\n",
    "X_test = testDataset[CNAME_DESCRIPTION].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(X_test))\n",
    "print(X_train[0], Y_train[0])\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22646b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "#print(train_vectors[0])\n",
    "\n",
    "Svm = CalibratedClassifierCV(LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "Svm.fit(train_vectors, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74155919",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.vocabulary_\n",
    "vectorizer_test = TfidfVectorizer(ngram_range=(1,2), vocabulary=voc)\n",
    "test_vectors = vectorizer_test.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19942c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Svm.predict(test_vectors)\n",
    "prediction_probas = Svm.predict_proba(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f71fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'predictions' + CNAME_VALID_FLAG + '.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "    for pred in predictions: \n",
    "        f.write(str(pred) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e009766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'prediction_probas' + CNAME_VALID_FLAG + '.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "    for proba in prediction_probas: \n",
    "        f.write(str(proba[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae89977c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be12c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
